{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 518生成新的資料庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests as r\n",
    "import sqlite3\n",
    "import lxml\n",
    "from bs4 import BeautifulSoup\n",
    "import math\n",
    "import time\n",
    "import pprint\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# index\n",
    "index = \"https://www.518.com.tw/job-index-P-1.html?i=1&am=1&ab=2032001,2032002,\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create table job518\n"
     ]
    }
   ],
   "source": [
    "# create table & drop table\n",
    "with sqlite3.connect('job518.sqlite') as conn:\n",
    "    c = conn.cursor()\n",
    "    try:\n",
    "        c.execute('drop table job518')\n",
    "        print('drop table job518')\n",
    "    except:\n",
    "        c.execute(\"\"\"CREATE TABLE job518(\n",
    "                    href TEXT PRIMARY KEY,\n",
    "                    info TEXT,\n",
    "                    lang TEXT)\"\"\")\n",
    "        print('create table job518')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of total job \n",
    "def getPage518(href):\n",
    "#     import locale\n",
    "#     locale.setlocale(locale.LC_ALL, 'en_US.UTF-8') \n",
    "    res = r.get(href)\n",
    "    \n",
    "    return int(BeautifulSoup(res.text, 'lxml').select('span.pagecountnum')[0].text.split('頁')[0].split('/')[1].strip())\n",
    "\n",
    "totalPages = getPage518(index)\n",
    "totalPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = r.get(index)\n",
    "soup = BeautifulSoup(res.text, 'lxml').select('div#listContent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.518.com.tw/MIS網管(駐點香港)-中國-香港區-job-1349006.html?kw=&pi=1'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test program -> get href\n",
    "soup[0].select('a')[0]['href'][0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sqlite insert&update -> job518\n",
    "def insert_href(href):\n",
    "    try:\n",
    "        with sqlite3.connect('job518.sqlite') as conn:\n",
    "\n",
    "            # Query histry, whether we have insert it before...\n",
    "            c = conn.cursor()\n",
    "            qryString = \"SELECT href FROM job518 where href=:href\"\n",
    "            c.execute(qryString, {'href' : href})\n",
    "            # if there are nothing like href\n",
    "            if len(c.fetchall()) == 0:\n",
    "                # insert new one\n",
    "                insert_string = \"INSERT INTO job518 (href, info) VALUES (?,'')\"\n",
    "                c.execute(insert_string, (href))\n",
    "            else:\n",
    "                update_string = \"UPDATE job518 SET times = times + 1 WHERE href = ?\"\n",
    "                c.execute(update_string, (href,))\n",
    "\n",
    "    except ConnectionError as e:\n",
    "        print(e)\n",
    "        print(href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-52-0a4c833076bc>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-52-0a4c833076bc>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    :\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# main program\n",
    "for page in range(1, totalPages + 1):\n",
    ":\n",
    "    href = \"https://www.518.com.tw/job-index-P-{}.html?i=1&am=1&ab=2032001,2032002,\".format(page)\n",
    "    soup = BeautifulSoup(r.get(href).text, 'lxml')\n",
    "    jidSoup = soup.select('div#listContent > ul')\n",
    "    totalJid = len(jidSoup)\n",
    "    print(totalJid)\n",
    "    print('page: ' + str(page))\n",
    "    for jid in range(0, totalJid-3):\n",
    "        title = jidSoup[jid].select('a')[0]['title'][0:].split('職缺名：')[1].split('\\n')[0]\n",
    "        href = jidSoup[jid].select('a')[0]['href'][0:].replace(' ','')\n",
    "        print(title)\n",
    "        print(href)\n",
    "        insert_href(title, href)\n",
    "    for jid in range(totalJid-3, totalJid):\n",
    "        title = jidSoup[jid].select('a')[0]['title'][0:]\n",
    "        href = jidSoup[jid].select('a')[0]['href'][0:].replace(' ','')\n",
    "        print(title)\n",
    "        print(href)\n",
    "        insert_href(title, href)\n",
    "    \n",
    "#     time.sleep(2)\n",
    "res.close()\n",
    "print('done!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import bs4\n",
    "import time\n",
    "import re\n",
    "import sqlite3\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定義 job104_for_info 方法\n",
    "# 參數為要爬的網頁的網址\n",
    "# 爬網頁之後解析出內文並回存進資料庫\n",
    "# 假設所需的套件都已經啟用\n",
    "\n",
    "\n",
    "def job518_for_info(href): #解析出內文並回存進資料庫\n",
    "    \n",
    "    \n",
    "    \n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        res = requests.get(href)\n",
    "        \n",
    "        soup = BeautifulSoup(res.text,'lxml')\n",
    "        job_content = soup.select('div.JobDescription')[0].text        #job_content\n",
    "        job_require = soup.select('div.job-detail-box > dl')[0].text   #job_require\n",
    "        d = (job_content + job_require)                                #將工作內容和工作需求合併\n",
    "        new_info = d.replace('\\n',' ').encode('ascii','ignore')        #並去除贅字轉換成ascii\n",
    "        new_info = new_info.decode('utf8')                             #在將文字格式改成string\n",
    "        \n",
    "        \n",
    "        with sqlite3.connect('job518.sqlite') as conn:\n",
    "                c = conn.cursor()\n",
    "                save_info = \"update job518 set info = ? where href = ?;\" \n",
    "                c.execute(save_info,(new_info,href))\n",
    "             \n",
    "            else:\n",
    "            print(\"外包網頁\")\n",
    "            \n",
    "    except IndexError as e:\n",
    "        print(e, href)\n",
    "\n",
    "    except:\n",
    "        print(href)\n",
    "        \n",
    "#爬蟲例外 例外方法寫在def裏面\n",
    "        \n",
    "class CrawlerError(Exception): \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 從資料庫找出需要抓的網頁的網址\n",
    "# 然後一筆一筆抓\n",
    "# 存進資料庫\n",
    "\n",
    "def update_None_info(): \n",
    "       \n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    import bs4\n",
    "    import time\n",
    "    import re\n",
    "    import sqlite3\n",
    "    \n",
    "    \n",
    "    \n",
    "    try:\n",
    "        with sqlite3.connect('job518.sqlite') as conn:\n",
    "\n",
    "            c = conn.cursor()\n",
    "            \n",
    "            # 找出info是空的url並找出網址list\n",
    "            \n",
    "            qryString = \"SELECT href FROM job518 where info is '';\" \n",
    "            c.execute(qryString)\n",
    "            \n",
    "            # 計算有多少筆完成\n",
    "            \n",
    "            do_number = 0 \n",
    "            lst = c.fetchall()\n",
    "            \n",
    "            \n",
    "    for a in lst:\n",
    "                # http://case.518.com.tw/casepage-detail-124929.html?from=518&a_id=2183\n",
    "                if a[0][0:15] != 'http://case.518': #若開頭網址不是case類型的,才進入迴圈\n",
    "                    job518_for_info(a[0]) \n",
    "                    do_number +=1\n",
    "                        print(do_number)\n",
    "                else:\n",
    "                    print(\"外包網頁\") #若出現則顯示外包網頁\n",
    "                \n",
    "            print('Has crawled {} info!'.format(do_number))\n",
    "            \n",
    "    except ConnectionError as e:\n",
    "        print(e)\n",
    "        print(a)\n",
    "        print(href)\n",
    "        \n",
    "    except CrawlerError as e:\n",
    "        print(a)\n",
    "        print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
